---
title: "Introduction to Linear Algebra"
number: "Lecture 1"
author: "DS-121"
format: 
    revealjs:
        css: styles.css
---

## Why Linear Algebra?   

::: {.antique-center-image}
![](/images/image1.jpeg)
:::

::: {.center-text}
Linear algebra, probability/statistics, and optimization are the mathematical pillars of machine learning.
:::


## Why Linear Algebra?
<span style="font-size: 0.8em;">
Linear algebra enables powerful machine learning techniques such as k-means for clustering, PageRank for web search ranking, PCA for dimensionality reduction, and neural networks for deep learning.
</span>

:::{.columns}
::: {.column width="50%"}
![](images/Clustering.png){fig-align="left" width=110%}
:::

::: {.column width="50%"}
![](images/page_rank.png){fig-align="right" width=110%}
:::
:::

## Why Linear Algebra?

:::{.columns}
::: {.column width="50%"}
![](images/PCA_genes_Europe.png){fig-align="left" width=110%}
:::

::: {.column width="50%"}
![](images/Handwritten_digits.png){fig-align="right" width=110%}
:::
:::

## What is Linear Algebra?

::: {.center-y}
:::{.incremental}
:::{.fragment}
<span style="font-size: 0.8em;">
Linear algebra is a branch of mathematics that deals with vectors, matrices, linear systems, and linear transformations. 
</span>

:::
:::{.fragment}
<span style="font-size: 0.8em;">
It involves understanding and manipulating multi-dimensional data through concepts like vector spaces, matrix operations, and matrix factorizations.
</span>

:::
:::
:::

## Three Modes of Thinking

:::{.incremental}
<span style="font-size: 0.8em;">
- **Algebraic thinking**: how to correctly manipulate symbols in a consistent logical framework.
</span>

<span style="font-size: 0.8em;">
- **Geometric thinking**: learning to extend familiar two- and three-dimensional concepts to higher dimensions.
</span>

<span style="font-size: 0.8em;">
- **Computational thinking**: understanding the relationship between abstract algebraic machinery and actual computations which efficiently arrive at the correct answer.
</span>

:::{.fragment}
<span style="font-size: 0.8em;">
Integrating these three concepts is _essential_ for using linear algebra in data science.
</span>
:::

:::

## Three Modes of Thinking
:::{center-text}
![](images/01-modes.jpg)
:::

## Three Modes of Vector ${\bf v}$
:::{.incremental}
<span style="font-size: 0.8em;">
- **Algebraically:** a vector $\mathbf{v}$ is an abstract object that permits addition ${\bf v} + {\bf w}$ and multiplication by a constant or *scalar* $c$ to form $c {\bf v}$.
</span>

<span style="font-size: 0.8em;">
- **Computationally:** a vector is an ordered list of real numbers.
</span>

<span style="font-size: 0.8em;">
  $$
  \large \begin{array}{ccc}
  {\bf u} = \left[\begin{array}{r}3\\-1\end{array}\right] &
  {\bf v} = \left[\begin{array}{c}2\\2\end{array}\right] &
  {\bf w} = \left[\begin{array}{c}-2\\-1\end{array}\right]
  \end{array}
  $$
</span>
:::

## Three Modes of Vector V
<span style="font-size: 0.8em;">
- **Geometrically:** a vector is a point in $n$-dimensional space.
</span>

:::{.center-text}
```{python}
import matplotlib.pyplot as plt
import laUtilities as ut

# Set up plot
ax = ut.plotSetup(size=(5,5))
ut.centerAxes(ax)

# Plot points
ut.plotPoint(ax, -2, -1)
ut.plotPoint(ax, 2, 2)
ut.plotPoint(ax, 3, -1)

# Draw arrows representing vectors
ax.arrow(0, 0, -2, -1, head_width=0.2, head_length=0.2, length_includes_head=True)
ax.arrow(0, 0, 2, 2, head_width=0.2, head_length=0.2, length_includes_head=True)
ax.arrow(0, 0, 3, -1, head_width=0.2, head_length=0.2, length_includes_head=True)

# Plot boundaries
ax.plot(0, -2, '')
ax.plot(-4, 0, '')
ax.plot(0, 2, '')
ax.plot(4, 0, '')

# Add labels
ax.text(3.25, -1.1, '$(3,-1)$', size=12)
ax.text(2.25, 1.9, '$(2,2)$', size=12)
ax.text(-3.5, -1.1, '$(-2,-1)$', size=12)

plt.show()
```
:::
## <span style="font-size: .8em;">Linear Algebra for Data Science</span>
<span style="font-size: 0.8em;">
Let us consider the well known Iris Dataset.
</span>

<!-- Make the image a little smaller and the code tab smaller -->
```{python}
from sklearn.datasets import load_iris
iris = load_iris()
rows, cols = iris.data.shape
print("This dataset contains", rows, "rows.")
print("Each row contains", cols, "columns that contain the following features:")
print(iris.feature_names)
```
:::{center-text}
![](images/01-iris.png)
:::

## <span style="font-size: .8em;">Linear Algebra for Data Science</span>
<span style="font-size: 0.8em;">
The data can be represented as a _matrix_.
</span>

:::{.columns}

:::{.column width="50%"}
```{python}
#|echo: true
A = iris.data
print(A.shape)

```
:::

:::{.column width="50%"}
```{python}
#|echo: true
print(A)
```
:::

:::

## <span style="font-size: .8em;">Linear Algebra for Data Science</span>

<span style="font-size: .8em;">This dataset might look random to the human eye. However, if we plot its attributes we can see that there is structure within the data.</span>

:::{.center-text}
```{python}
import matplotlib.pyplot as plt
#|echo: true
x_index = 2
y_index = 3 
# this formatter will label the colorbar with the correct target names
formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])
plt.scatter(A[:, x_index], A[:, y_index], c=iris.target, cmap = plt.cm.get_cmap('RdYlBu', 3))
plt.colorbar(ticks=[0, 1, 2], format=formatter)
plt.clim(-0.5, 2.5)
plt.xlabel(iris.feature_names[x_index])
plt.ylabel(iris.feature_names[y_index]);
```
:::

## <span style="font-size: .8em;">Linear Algebra for Data Science</span>

<span style="font-size: .8em;">
We can make the following observations.
</span>


<span style="font-size: .8em;">
1. There are _correlations_ among the 150 rows of A. 
</span>

<span style="font-size: .8em;">
- Knowing the petal length of an iris tells you something about the petal width. 
- Knowing the petal length and width might give you a way to classify which type of iris you have.
</span>

<span style="font-size: .8em;">
2. The matrix is _compressible_.
</span>

<span style="font-size: .8em;">
  - The 150 rows of $A$ are all samples of the same underlying physical process.<br>
  - We should be able to decompose this $150 \times 4$ matrix into simpler matrices.
</span>

## <span style="font-size: .8em;">Linear Algebra for Data Science</span>

<span style="font-size: 0.8em;">
What makes a matrix _simple_?
</span>


:::{.incremental}
<span style="font-size: 0.8em;">
- **Size:** one matrix might have fewer rows or columns than another, which means it is more compact and has fewer degrees of freedom.
</span>

<span style="font-size: 0.8em;">
- **Sparsity:** a matrix might be lower triangular (denoted $L$) or upper triangular (denoted $U$ or $R$), with all other entries equal to zero.
</span>

<span style="font-size: 0.8em;">
- **Symmetry:** the entries of the matrix might be symmetric across the diagonal $S$, or it might even be a diagonal matrix $\Sigma$ with all other entries being 0.
</span>
:::

## Matrix Decomposition
<span style="font-size: 0.8em;">
- $A = L U$ - Gauss-Jordan Elimination <br>
- $A = Q R$ - Gram-Schmidt orthogonalization <br>
- $A = U \Sigma V$ - Singular Value Decomposition or SVD
</span>

:::{.center-text}
![](images/01-svd.png)
:::

## Assumptions
<span style="font-size: 0.8em;">
Many techniques in this course involve dealing with high-dimensional datasets for which we assume low intrinsic dimensionality. This is often a good assumption, but not always!
</span>

:::{.center-text}
![](images/01-spurious.png)
:::