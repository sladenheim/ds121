---
title: "Principal Component Analysis"
author: "DS 121"
format: 
    revealjs:
        css: styles.css
---

## High Dimensional Data

::: {style="font-size: 0.8em;"}
:::{.columns}
::: {.column width="50%"}
![](/images/pca/Extended_Yale_B_DB_cropped.png)
:::
::: {.column width="50%" style="height: 80%; padding-top: 20vh;"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p>How are greyscale images typically stored in a computer?</p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::
:::
:::

## High Dimensional Data

::: {style="font-size: 0.8em;"}
:::{.columns}
::: {.column width="50%"}
![](/images/pca/Extended_Yale_B_DB_cropped.png)
:::
::: {.column width="50%" style="padding-top: 15vh;"}
::: {.incremental}
- **Database:** $64$ images for $38$ people <br>$\Rightarrow 2432$ images
- **Image size:** $192 \times 168$ pixels <br>$\Rightarrow 32256$ pixels
- **Each Pixel:** $0-255$
:::
:::
::: {.incremental}
::: {.fragment .center-text}
In matrix format: $2432$ rows $\times$ $32256$ columns
:::
:::
:::
:::

## High Dimensional Data

::: {style="font-size: 0.8em; "}

High Dimensional Data contains a high number of features/columns for each observation. <br><br>

Additional examples: biomedical, educational, or financial records. <br><br>

:::{.center-text}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p>What is problematic about high dimensional data?</p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::
:::

## High Dimensional Data
:::{.center-text style="max-height: 40%; max-width: 40%; margin-bottom: 1rem;"}
![](/images/pca/elephant_perspective.png)
:::
::: {style="font-size: 0.8em;"}
Potential Challenges:

:::{.incremental}
- The curse of dimensionality
- Overfitting
- High computational complexity
- Difficulties with visualization and interpretation
:::
:::

## Dimensionality Reduction

::: {style="font-size: 0.8em;"}

:::{.fragment }
PCA allows to reduce the dimension of the data while preserving the most important information.
:::

:::{.incremental}

:::{.fragment }
Let us write this in a more mathematical way:
:::

:::{.fragment }
**Input:**  $\mathbf{x}_1,\ldots, \mathbf{x}_m$ _with_  $\mathbf{x}_i \in \mathbb{R}^n \: \: \forall \: i \in \{1, \ldots, m\}.$<br>
**Output:** $\mathbf{y}_1,\dots, \mathbf{y}_m$  _with_  $\mathbf{y}_i \in \mathbb{R}^k \: \: \forall \: i \in \{1, \dots, m\},$ _where_ 
[$k \ll n$]{style="color:red;"}.<br><br>
:::

:::{.fragment}
Data points $\mathbf{y}_i$ have a lower dimension, but preserve the most information contained in data points $\mathbf{x}_i$.
:::
:::
:::

## Dimensionality Reduction

::: {style="font-size: 0.8em; "}
:::{.incremental}
:::{.fragment}
In matrix form we can denote this as:

:::{style="transform: scale(.8, 0.7); margin: -1.5em;"}
$$
X_0 =
\begin{bmatrix}
\text{—} & \mathbf{x}_1 & \text{—} \\
\text{—} & \mathbf{x}_2 & \text{—} \\
\text{—} & \vdots & \text{—} \\
\text{—} & \mathbf{x}_m & \text{—}
\end{bmatrix}
\:
\rightarrow \:
Y =
\begin{bmatrix}
\text{—} & \mathbf{y}_1 & \text{—} \\
\text{—} & \mathbf{y}_2 & \text{—} \\
\text{—} & \vdots & \text{—} \\
\text{—} & \mathbf{y}_m & \text{—}
\end{bmatrix}.
$$
:::
:::
:::

:::{.fragment}
Equivalently, we can write:

:::{style="transform: scale(.8, 0.7); margin: -1.5em;"}
$$
X_0 = 
\begin{bmatrix} 
x_{11} & x_{12} & \dots & x_{1n} \\
x_{21} & x_{22} & \dots & x_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{m1} & x_{m2} & \dots & x_{mn} 
\end{bmatrix} \:
\rightarrow \:
Y = \begin{bmatrix} 
y_{11} & y_{12} & \dots & y_{1k} \\
y_{21} & y_{22} & \dots & y_{2k} \\
\vdots & \vdots & \ddots & \vdots \\
y_{m1} & y_{m2} & \dots & y_{mk} 
\end{bmatrix}.
$$
:::
:::
:::

## PCA Application: Genes Mirror Geography

::: {style="font-size: 0.8em; "}
:::{.columns}

:::{.column width="50%" style="font-size: 1em;"}
_Single Nucleotide Polymorphism (SNP)_ describes the change in DNA from common base pairs (A,T,C,G).<br><br>
**Data:** DNA material (SNP's) of 3000 Europeans.<br><br>
**Dimensions:** $3000 \times 500k$.
:::
:::{.column width="50%"}
![](/images/pca/PCA_genes_Europe.png)
:::
:::
:::

## PCA Application: Genes Mirror Geography

::: {style="font-size: 0.8em;"}
::: {.fragment }
Key Observations:

:::{.incremental}
- The first principal components almost reproduce the map of Europe.
- SNPs are similar geographically.
- DNA of an individual reveals their birthplace within a few hundred kilometers.
:::

:::{.fragment .center-text}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p>Would a similar study in the USA be effective?</p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::
:::
:::
<!-- Had to do this slide a little janky. : - ( -->
## PCA Overview
::: {style="font-size: 0.8em; "}
Input:  $X_0 \in \mathbb{R}^{m \times n}$.<br>
Output: $Y \in \mathbb{R}^{m \times k}$ with $k \ll n$.

::::{style="font-size: 1em"}

:::{.fragment}
Step **1**: Center the data (i.e., subtract the mean across rows): $X_0 \rightarrow X$.
:::
:::{.fragment}
Step **2**: Find the direction of the largest variance: direction of the first principal component, PC 1.
:::

:::{.fragment}
Step **3**: From the directions <br> orthogonal to that of PC 1 find <br> one with the largest variance:<br> direction of PC 2.
:::
:::{.fragment}
Step **4**: ...
![](/images/pca/PCs_variance.png){style="position: absolute; bottom: .2rem; right: 6rem; width: 40%; height: 40%;"}
:::
:::{.fragment}
Step **5**: Project the data into the <br>directions of the PCs.
:::
:::

::::

## Geometric Interpretation

::: {style="font-size: .8em"}
Centered data often clusters along a line or other low-dimensional <br> subspace of $\mathbb{R}^{n}$.
:::
:::: {.columns}
::: {.column width="50%" style="text-align: center;"}
![](images/pca/PCA_residual.png){style="width: 60%; height: auto; margin: 0;"}
<p style="text-align: left; font-size: .8em">
Find a line (or k-dimensional hyperplane) that is closest to the points: <b>minimizes</b> the sum of squared distances from the points to the line.</p>
:::

::: {.column width="50%" style="text-align: center;"}
![](images/pca/PCA_variance.png){style="width: 58%; height: auto; margin: 0;"}

<p style="text-align: left; font-size: .8em">
Find a line (or k-dimensional hyperplane) that captures the most variance in the data: <b>maximizes</b> the sum of squared distances to the mean.
</p>

:::
::::

## Geometric Interpretation

![](/images/pca/pca.gif)

## Geometric Interpretation

::::{.columns}

:::{.column width="33%"}
```{python}
import numpy as np
import matplotlib.pyplot as plt
import laUtilities as ut

n_samples = 500
C = np.array([[0.1, 0.6], [2., .6]])
X = np.random.randn(n_samples, 2) @ C + np.array([-6, 3])
ax = plt.figure(figsize = (7, 7)).add_subplot()
plt.xlim([-12, 12])
plt.ylim([-7, 7])
ut.centerAxes(ax)
plt.axis('equal')
plt.scatter(X[:, 0], X[:, 1], s=10, alpha=1)
```
:::

:::{.column width="33%"}
```{python}
Xc = X - np.mean(X,axis=0)
ax = plt.figure(figsize = (7, 7)).add_subplot()
plt.xlim([-12, 12])
plt.ylim([-7, 7])
ut.centerAxes(ax)
plt.axis('equal')
plt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)
plt.scatter(Xc[:, 0], Xc[:, 1], s=10, alpha=0.8, color='r')
```
:::

:::{.column width="33%"}
```{python}
Xc = X - np.mean(X, axis = 0)
u, s, vt = np.linalg.svd(Xc, full_matrices=False)

scopy = s.copy()
scopy[1] = 0.
reducedX = u @ np.diag(scopy) @ vt


ax = plt.figure(figsize = (7, 7)).add_subplot()
ut.centerAxes(ax)
plt.axis('equal')
plt.scatter(Xc[:,0],Xc[:,1], color='r')
plt.scatter(reducedX[:,0], reducedX[:,1])
endpoints = np.array([[-10],[10]]) @ vt[[0],:]
plt.plot(endpoints[:,0], endpoints[:,1], 'g-');
```
:::
::::

## Covariance Matrix
:::{style="font-size: 0.8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="purple-box" style="background-color: #f3e6ff; padding: 10px; border-radius: 5px;">
        <p>
            Let \\( X \\in \\mathbb{R}^{m \\times n} \\) contain the centered data.
            Then the sample covariance matrix is defined by:
        </p>
        <p style="text-align: center; font-size: 1em;">
            \\[
            S = \\frac{1}{m-1} XX^T.
            \\]
        </p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
$S$ is symmetric:

$$
S^T = \left( \frac{1}{m-1} X X^T\right)^T = \frac{1}{m-1}  (X^T)^T X^T = \frac{1}{m-1} X X^T = S.
$$

Since $S$ is symmetric, we can apply the **spectral decomposition**.
:::

## Spectral Decomposition
:::{style="font-size: .8em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="purple-box" style="background-color: #f3e6ff; padding: 2px; border-radius: 5px;">
        <p>
            Every symmetric matrix \( S \) has the factorization \(S = V \Lambda V^T,\) where \( \Lambda \) is a diagonal matrix that contains the eigenvalues of \( S \) and \( V \) is a matrix whose columns are orthonormal eigenvectors of \( S \).
        </p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```

The covariance matrix $S \in \mathbb{R}^{n \times n}$ is decomposed into $S = V \Lambda V^T,$ where

$$
\Lambda = 
\begin{bmatrix}
\lambda_1                                   \\
& \lambda_2             &   & \Large0 \\
  &               & \ddots                \\
% & \Large0 &   & \lambda_{n-1}            \\
 &        \Large0       &   &   & \lambda_n
\end{bmatrix}
\quad \text{and} \quad
V = \begin{bmatrix} 
\bigg| & \bigg| &   & \bigg| \\
\mathbf{v}_1   & \mathbf{v}_2  & \dots  & \mathbf{v}_n  \\
\bigg| & \bigg| & & \bigg|
\end{bmatrix}
$$

with $\lambda_1\geq\lambda_2\geq\dots\geq\lambda_n$ and $S\mathbf{v}_i=\lambda_i\mathbf{v}_i$, $\mathbf{v}_i\perp\mathbf{v}_j$ $(i\neq j)$, $||\mathbf{v}_i|| = 1$.


:::

## Essentials of PCA

:::{.incremental style="font-size: .8em"}
- The columns of $V$ are the principal directions.
- The principal components are the projection of the data into principal directions: $X^TV$.
- The total variance $T$ in the data is the sum of all eigenvalues: $T=\lambda_1+\lambda_2+\cdots+\lambda_n$.
- The first eigenvector $\mathbf{v}_1$ points in the most significant direction of the data. This direction explains the largest fraction $\lambda_1/T$ of the total variance.
- The second eigenvector $\mathbf{v}_2$ accounts for a smaller fraction $\lambda_2/T$.
:::

## Example

:::{style="font-size: .8em"}
:::{.fragment}
Consider the following dataset $\mathbf{x}_1 = \begin{bmatrix} 90 \\ 60 \\ 60 \end{bmatrix}$ and $\mathbf{x}_2 = \begin{bmatrix} 80 \\ 60 \\ 70 \end{bmatrix}$. <br>Let us find its **first principal component**.
:::
:::{.fragment}
First, we construct $X_0$: $X_0 = \begin{bmatrix} 90 & 60 & 60 \\ 80 & 60 & 70 \end{bmatrix}$.
:::
:::{.fragment}
The mean across rows is: $\mu = \begin{bmatrix} 85 & 60 & 65 \end{bmatrix}$.
:::
:::{.fragment}
:::{style="font-size: 1em"}
We **center** the data by subtracting each entry of $\mu$ from the corresponding column in $X_0$:
:::
:::{.center-text}
$X = \begin{bmatrix} 5 & 0 & -5 \\ -5 & 0 & 5 \end{bmatrix}$.
:::
:::
::: 

## Example

::::{style="font-size: .8em"}
Next, we compute the **covariance matrix:**

$$
S = \frac{1}{2-1}XX^T = 
\begin{bmatrix} 5 & 0 & -5 \\ -5 & 0 & 5 \end{bmatrix} \begin{bmatrix} 5 & -5 \\ 0 & 0 \\ -5 & 5 \end{bmatrix}
= \begin{bmatrix} 50 & -50 \\ -50 & 50 \end{bmatrix}.
$$

The eigenvalues of $S$ are $\lambda_1=100$ and $\lambda_2=0$.

The eigenvectors of $S$ are:

$$
\mathbf{v}_1 = \begin{bmatrix} 0.7071 \\ -0.7071 \end{bmatrix},
\mathbf{v}_2 = \begin{bmatrix} 0.7071 \\ 0.7071 \end{bmatrix}.
$$
:::

## Example

::::{style="font-size: .8em"}
:::{.center-text style="font-size: 1em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p>What is the total variance in the data?</p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.fragment}
The **total variance** is $T=100$.
:::

:::{.fragment}
This implies that the first principal component accounts for all the variance in the dataset.
:::
:::{.fragment}
We can compute the first principal component as:

$$
Y = X^T\mathbf{v}_1 = \begin{bmatrix} 5 & -5 \\ 0 & 0 \\ -5 & 5 \end{bmatrix}
\begin{bmatrix} 0.7071 \\ -0.7071 \end{bmatrix}
= \begin{bmatrix} 7.071 \\ 0 \\ -7.071 \end{bmatrix}.
$$
:::
:::

<!-- ## Example

:::{style="font-size: .8em"}
:::{.fragment}
Consider the following dataset $\mathbf{x}_1 = \begin{bmatrix} 90 \\ 60 \\ 60 \end{bmatrix}$ and $\mathbf{x}_2 = \begin{bmatrix} 80 \\ 60 \\ 70 \end{bmatrix}$. <br>Let us find its **first principal component**.
:::
:::{.fragment}
First, we construct $X_0$: $X_0 = \begin{bmatrix} 90 & 60 & 60 \\ 80 & 60 & 70 \end{bmatrix}^T$. Note the use of the transpose.
:::
:::{.fragment}
The mean across columns is: $\mu = \begin{bmatrix} 85 & 60 & 65 \end{bmatrix}^T$.
:::
:::{.fragment}
:::{style="font-size: 1em"}
We **center** the data by subtracting each entry of $\mu$ from the corresponding column in $X_0$:
:::
:::{.center-text}
$X = \begin{bmatrix} 5 & 0 & -5 \\ -5 & 0 & 5 \end{bmatrix}^T$.
:::
:::
::: 

## Example

::::{style="font-size: .8em"}
Next, we compute the **covariance matrix:**

$$
S = \frac{1}{2-1}X^TX = 
\begin{bmatrix} 5 & 0 & -5 \\ -5 & 0 & 5 \end{bmatrix}
\begin{bmatrix} 5 & -5 \\ 0 & 0 \\ -5 & 5 \end{bmatrix}
= \begin{bmatrix} 50 & -50 \\  -50 & 50 \end{bmatrix}.
$$

The eigenvalues of $S$ are $\lambda_1=100$ and $\lambda_2=0$.

The eigenvectors of $S$ are:

$$
\mathbf{v}_1 = \begin{bmatrix} 0.7071 \\ -0.7071 \end{bmatrix},
\mathbf{v}_2 = \begin{bmatrix} 0.7071 \\ 0.7071 \end{bmatrix}.
$$
:::

## Example

::::{style="font-size: .8em"}
:::{.center-text style="font-size: 1em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p>What is the total variance in the data?</p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::

:::{.fragment}
The **total variance** is $T=100$.
:::

:::{.fragment}
This implies that the first principal component accounts for all the variance in the dataset.
:::
:::{.fragment}
We can compute the first principal component as:

$$
Y = X\mathbf{v}_1 = \begin{bmatrix} 5 & -5 \\ 0 & 0 \\ -5 & 5 \end{bmatrix}
\begin{bmatrix} 0.7071 \\ -0.7071 \end{bmatrix}
= \begin{bmatrix} 7.071 \\ -7.071 \end{bmatrix}.
$$
:::
::: -->


## PCA Step by Step
:::{style="font-size: .8em"}
Input: $X_0 \in \mathbb{R}^{m \times n}$. Output: $Y \in \mathbb{R}^{m \times k}$ with $k \ll n.$

:::{.fragment}

1. Center the data: $X_0 \to X$.
:::

:::{.fragment}
2. Compute the sample covariance matrix: $S = \frac{1}{m-1}XX^T$.
:::

:::{.fragment}
3. Compute the eigenvalues $\lambda_i$'s and the eigenvectors $\mathbf{v}_i$'s of $S$.
:::

:::{.fragment}
4. Select $k$ eigenvectors corresponding to the highest eigenvalues.
:::

:::{.fragment}
5. Assemble $V_k = \begin{bmatrix} | & | &  & | \\\mathbf{v}_1 & \mathbf{v}_2 & \dots & \mathbf{v}_k \\| & | & & | \end{bmatrix}$.
:::

:::{.fragment}
6. Project the data: $Y = X^TV_k$.
:::
:::

## Relationship to the SVD

:::{style="font-size: .8em;"}
The SVD of a mean-centered data matrix $X \in \mathbb{R}^{m \times n}$ is $X = U \Sigma V^T$.
<!-- Had to use an image instead of python code because of whitespace created -->
![](/images/pca/UeV.png)

:::{.center-text}
Hence, $XX^T = (U\Sigma V^T)(U\Sigma V^T)^T = U\Sigma V^T(V^T)^T\Sigma U^T$
:::
$$
= U\Sigma V^TV\Sigma U^T = U\Sigma I\Sigma U^T = U\Sigma^2 U^T.
$$
:::

## Relationship to the SVD
:::{style="font-size: 0.8em;"}
:::{.fragment}
Recall that in PCA the covariance matrix $S$ is given by $\frac{1}{m-1} XX^T$.
:::
<br>

:::{.fragment}
Thus, $S = \frac{1}{m-1} U \Sigma^2 U^T$.
:::
<br>

:::{.fragment}
This tells us that the eigenvalues $\lambda_i$ of $S$ are the squares of the singular values $\sigma_i^2$ of $\Sigma$, scaled by $\frac{1}{m-1}$.
:::

:::{.fragment}
In practice, PCA is done by computing the SVD of your data matrix as opposed to forming the covariance matrix and computing the eigenvalues. 
::: 

:::


## Case Study: Random Data
:::{style="font-size: 0.8em; "}
Let us consider some randomly generated data consisting of 2 features.

```{python}
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt

rng = np.random.default_rng(seed=42)

import numpy as np

# Set the seed for reproducibility
seed = 42
rng = np.random.default_rng(seed)

n_samples = 500
C = np.array([[0.1, 0.6], [2., .6]])
X0 = rng.standard_normal((n_samples, 2)) @ C + np.array([-6, 3])
X = X0 - X0.mean(axis=0)

plt.scatter(X[:, 0], X[:, 1])
plt.axis('equal')
plt.show()
```
:::
---

## Case Study: Random Data
:::{style="font-size: 0.8em; "}
Let us do PCA and plot the directions of the principle components over our dataset. As expected, the principal components are orthogonal and point in the directions of the maximum variance.

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt

origin = [0, 0]

# Compute the covariance matrix
cov_matrix = np.cov(X, rowvar=False)

# Calculate the eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

# Sort the eigenvalues and eigenvectors in descending order
sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]

# Plot the original dataset
plt.scatter(X[:, 0], X[:, 1], alpha=0.2)

# Plot the principal components
for i in range(len(eigenvalues)):
    plt.quiver(origin[0], origin[1], -eigenvectors[0, i], -eigenvectors[1, i],
               angles='xy', scale_units='xy', scale=1, color=['r', 'g'][i])

plt.title('Principal Components on Original Dataset')
plt.axis('equal')
plt.show()
```
:::

## Case Study: Digits
:::{style="font-size: 0.8em; "}
Let us consider another example using the MNIST dataset.

```{python}
#| fig-align: center
from sklearn import datasets
digits = datasets.load_digits()

plt.figure(figsize=(8, 8),)
for i in range(8):
    plt.subplot(2, 4, i + 1)
    plt.imshow(digits.images[i], cmap='gray_r')
    plt.axis('off')
plt.tight_layout()
plt.show()
```
:::
---
## Case Study: Digits
:::{style="font-size: .8em; "}
We first stack the columns of each digit on top of each other and perform PCA on the 64-D representation of the digits.
:::
```{python}
#| fig-align: center
from sklearn.decomposition import PCA

X = digits.data
y = digits.target

pca = PCA()
X_pca = pca.fit_transform(X)

# Create subplots
fig, axs = plt.subplots(2, 1, figsize=(6, 5))

# Scree plot (graph of eigenvalues corresponding to PC number)
# This shows the explained variance ratio
axs[0].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')
axs[0].set_title('Scree Plot')
axs[0].set_xlabel('Principal Component')
axs[0].set_ylabel('Explained Variance Ratio')
axs[0].grid(True)

# Cumulative explained variance plot
axs[1].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')
axs[1].set_title('Cumulative Explained Variance Plot')
axs[1].set_xlabel('Principal Component')
axs[1].set_ylabel('Cumulative Explained Variance')
axs[1].grid(True)

# Adjust layout
plt.tight_layout()
plt.show()
```

---

:::{style="font-size: .8em"}
Let us plot the data in the first 2 principal component directions. We'll use the digit labels to color each digit in the reduced space.
:::

```{python}
#| fig-align: center
plt.figure(figsize=(7, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab20', edgecolor='k', s=50)
plt.title('Digits in PC1 and PC2 Space')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

# Create a legend with discrete labels
legend_labels = np.unique(y)
handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab20(i / 9), markersize=10) for i in legend_labels]
plt.legend(handles, legend_labels, title="Digit Label", loc="best")

plt.show()
```


---

## Case Study: Digits
:::{style="font-size: .8em; padding-top: 10vh;"}
We observe the following in our plot of the digits in the first two principal components:

- There is a decent clustering of some of our digits, in particular 0, 2, 3, 4, and 6.
- There is a clear difference between certain digits (i.e., 0 and 1, and 3 and 4).
- The numbers 0 and 6 seem to be relatively close to each other in this space.
- There is not a very clear separation of the number 5 from some of the other points.
:::

## To Scale Or Not To Scale
:::{style="font-size: .8em"}
Consider a situation where we have age and height data for 4 people.

:::{.center-text style="max-width:80%;"}
:::: {.columns}
::: {.column width="50%"}
| Person | Age [years] | Height [feet] |
|--------|-------------|---------------|
| A      | 25          | 6.232         |
| B      | 30          | 6.232         |
| C      | 25          | 5.248         |
| D      | 30          | 5.248         |

:::
::: {.column width="50%"}
![](images/Scaling_feet.png)
:::
::::
:::

::::{.fragment}
:::{.center-text}
Data varies most in the **horizontal direction.**
:::
:::
::::

## To Scale Or Not To Scale
:::{style="font-size: .8em"}
What if the height is in cm?

:::{.center-text style="max-width:80%;"}
:::: {.columns}
::: {.column width="50%"}
| Person | Age [years] | Height [cm] |
|--------|-------------|-------------|
| A      | 25          | 189.95      |
| B      | 30          | 189.95      |
| C      | 25          | 159.96      |
| D      | 30          | 159.96      |

:::
::: {.column width="50%"}
![](images/Scaling_cm.png)
:::
::::
:::

::::{.fragment}
:::{.center-text}
Data varies most in the **vertical direction.**
:::
:::
::::

## To Scale Or Not To Scale
:::{style="font-size: .8em"}
Let's standardize our data.

:::{.center-text style="max-width:80%;"}
:::: {.columns}
::: {.column width="50%"}
| Person | Age [years] | Height [cm] |
|--------|-------------|-------------|
| A      | -1          | 1           |
| B      | 1           | 1           |
| C      | -1          | -1          |
| D      | 1           | -1          |

:::
::: {.column width="50%"}
![](images/Scaling.png)
:::
::::
:::

::::{.fragment}
:::{.center-text}
When we normalize our data, we observe an **equal distribution** of the variance.
:::
:::
::::

## Eigenfaces
:::{style="font-size: .8em"}

:::{.center-text style="max-width:80%;"}
:::: {.columns}
::: {.column width="68%"}
- PCA provides a set of standardized facial features.
- $X_0$ is a training set with a wide variety of faces.
- Eigenvector $\textbf{v}_1$ tells the best combination of known faces that best describe a new face, $\textbf{v}_2$ is the next best combination.
- First commercial use: Super Bowl 35 in Tampa.
- Eigenvoices, Eigeneyes, Eigenexpressions, ...


:::
::: {.column width="32%"}
![](images/Eigenfaces.png)
Eigenfaces pick out hairline, mouth, eyes, and shape.
:::
::::
:::

