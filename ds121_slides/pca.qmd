---
title: "Principal Component Analysis"
author: "Lisa Wobbes"
format: 
    revealjs:
        css: styles.css
---

## High Dimensional Data

:::{.columns}
::: {.column width="50%"}
![](/images/pca/Extended_Yale_B_DB_cropped.png)
:::
::: {.column width="50%" style="height: 80%; padding-top: 10vh;"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p>How are greyscale images typically fit in a computer?</p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::
:::

## High Dimensional Data

:::{.columns}
::: {.column width="50%"}
![](/images/pca/Extended_Yale_B_DB_cropped.png)
:::
::: {.column width="50%"}
::: {.incremental}
- **Database:** 64 images for 38 people <br>$\Rightarrow 2432$ images
- **Image size:** $192 \times 168$ pixels <br>$\Rightarrow 32256$ pixels
- **Each Pixel:**<br>$0-255$
:::
:::
::: {.incremental}
::: {.fragment .center-text}
In **_matrix_** format: $2432$ rows $\times$ $32256$ columns
:::
:::
:::

## High Dimensional Data

High Dimensional Data contains a high number of features/columns for each observation. <br><br>

Additional examples: biomedical, educational, or financial records. <br><br>

:::{.center-text}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p><b>What is problematic about high dimensional data?</b></p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::

## High Dimensional Data
:::{.center-text style="max-height: 40%; max-width: 40%; margin-bottom: 1rem;"}
![](/images/pca/elephant_perspective.png)
:::
::: {style="font-size: 0.8em;"}
Potential Challenges:

:::{.incremental}
- The curse of dimensionality
- Overfitting
- High Computational Complexity
- Difficulties with visualization and interpretation
:::
:::

## Dimensionality Reduction

:::{.center-text }
**PCA** allows to reduce the dimension of the data while preserving the most important information.
:::

:::{.incremental}

:::{.fragment style="font-size: .8em"}
Let us write this in a more mathematical way:<br>

**Input:** $\bar{\mathbf{x}}_1,\ldots, \bar{\mathbf{x}}_m$ _with_  $\bar{\mathbf{x}}_i \in \mathbb{R}^n \: \: \forall \: i \in \{1, \ldots, n\}.$<br>
**Output:** $\bar{\mathbf{y}}_1,\dots, \bar{\mathbf{y}}_m$  _with_  $\bar{\mathbf{y}}_i \in \mathbb{R}^d \: \: \forall \: i \in \{1, \dots, n\},$ _where_ $k \ll n$.<br><br>
:::

:::{.fragment}
Datapoints $\bar{\mathbf{y}}_i$ have a _lower dimension_, but preserve the _most information_ contained in in data points $\bar{\mathbf{x}}_i$.
:::

:::

## Dimensionality Reduction

:::{.incremental}
:::{.fragment}
In matrix form we can denote this as:

:::{style="transform: scale(.8, 0.7); margin: -1.5em;"}
$$
X_0 =
\begin{bmatrix}
\text{—} & \bar{\mathbf{x}}_1 & \text{—} \\
\text{—} & \bar{\mathbf{x}}_2 & \text{—} \\
\text{—} & \vdots & \text{—} \\
\text{—} & \bar{\mathbf{x}}_m & \text{—}
\end{bmatrix}
\:
\xrightarrow[\text{PCA}]{} \:
Y =
\begin{bmatrix}
\text{—} & \bar{\mathbf{y}}_1 & \text{—} \\
\text{—} & \bar{\mathbf{y}}_2 & \text{—} \\
\text{—} & \vdots & \text{—} \\
\text{—} & \bar{\mathbf{y}}_m & \text{—}
\end{bmatrix}
$$
:::
:::

:::{.fragment}
_Equivalently_, we can write:

:::{style="transform: scale(.8, 0.7); margin: -1.5em;"}
$$
X_0 = 
\begin{bmatrix} 
x_{11} & x_{12} & \dots & x_{1n} \\
x_{21} & x_{22} & \dots & x_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{m1} & x_{m2} & \dots & x_{mn} 
\end{bmatrix} \:
\xrightarrow[\text{PCA}]{} \:
Y = \begin{bmatrix} 
y_{11} & y_{12} & \dots & y_{1d} \\
y_{21} & y_{22} & \dots & y_{2d} \\
\vdots & \vdots & \ddots & \vdots \\
y_{m1} & y_{m2} & \dots & y_{md} 
\end{bmatrix}
$$
:::
:::
:::

## PCA Application: Genes Mirror Geography

:::{.columns}

:::{.column width="50%" style="font-size: 0.8em;"}
_Single Nucleotide Polymorphism (SNP)_ describes the change in DNA from common base pairs (A,T,C,G).<br><br>
**Data:** DNA material (SNP's) of 3000 Europeans.<br><br>
**Dimensions:** $3000 \times 500k$.
:::
:::{.column width="50%"}
![](/images/pca/PCA_genes_Europe.png)
:::
:::

## PCA Application: Genes Mirror Geography

::: {style="font-size: .75em"}
Key Observations:

:::{.incremental}
- the _first principal components_ almost reproduce the map of Europe;
- SNPs are similar geographically;
- DNA of an individual reveals their birthplace within a few hundred kilometers.
:::

:::{.fragment .center-text}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p><b>Would a similar study in the USA be effective?</b></p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::
:::
<!-- Had to do this slide a little janky. : - ( -->
## PCA Overview
_Input_:  $\mathbf{x}_1,\dots,\mathbf{x}_n \in \mathbb{R}^D$ <br>
_Output_: $\mathbf{y}_1,\dots,\mathbf{y}_n \in \mathbb{R}^d$ with $d << D$

::::{style="font-size: .8em"}

:::{.fragment}
Step **1**: Center the data (i.e., subtract the mean across rows): $\mathbf{x}_1 \rightarrow \mathbf{x}$.
:::
:::{.fragment}
Step **2**: Find the direction of the largest variance: direction of PC 1.
:::

:::{.fragment}
Step **3**: From the directions orthogonal to that of PC 1 find one with the largest variance: direction of PC 2.
:::
:::{.fragment}
Step **4**: ...
![](/images/pca/PCs_variance.png){style="position: absolute; bottom: .2rem; right: 12rem; width: 40%; height: 40%;"}
:::
:::{.fragment}
Step **5**: Project the data into the <br>directions of the PCs.
:::

::::

## Least Squares Interpretation

::: {style="font-size: .75em"}
Centered data often clusters along a line or other low-dimensional subspace of $\mathbb{R}^{n}$.
:::
:::: {.columns}
::: {.column width="50%" style="text-align: center;"}
![](images/pca/PCA_residual.png){style="width: 60%; height: auto; margin: 0;"}
<p style="text-align: left; font-size: .75em">
Find a line <em>(or k-dimensional hyperplane)</em> that is closest to the points: <b>minimizes</b> the sum of squared distances from the points to the line.</p>
:::

::: {.column width="50%" style="text-align: center;"}
![](images/pca/PCA_variance.png){style="width: 60%; height: auto; margin: 0;"}

<p style="text-align: left; font-size: .75em">
Find a line <em>(or k-dimensional hyperplane)</em> that captures the most variance in the data: <b>maximizes</b> the sum of squared distances to the mean.
</p>

:::
::::

## Geometric Interpretation

![](/images/pca/pca.gif)

## Geometric Interpretation

::::{.columns}

:::{.column width="33%"}
```{python}
import numpy as np
import matplotlib.pyplot as plt
import laUtilities as ut

n_samples = 500
C = np.array([[0.1, 0.6], [2., .6]])
X = np.random.randn(n_samples, 2) @ C + np.array([-6, 3])
ax = plt.figure(figsize = (7, 7)).add_subplot()
plt.xlim([-12, 12])
plt.ylim([-7, 7])
ut.centerAxes(ax)
plt.axis('equal')
plt.scatter(X[:, 0], X[:, 1], s=10, alpha=1)
```
:::

:::{.column width="33%"}
```{python}
Xc = X - np.mean(X,axis=0)
ax = plt.figure(figsize = (7, 7)).add_subplot()
plt.xlim([-12, 12])
plt.ylim([-7, 7])
ut.centerAxes(ax)
plt.axis('equal')
plt.scatter(X[:, 0], X[:, 1], s=10, alpha=0.8)
plt.scatter(Xc[:, 0], Xc[:, 1], s=10, alpha=0.8, color='r')
```
:::

:::{.column width="33%"}
```{python}
Xc = X - np.mean(X, axis = 0)
u, s, vt = np.linalg.svd(Xc, full_matrices=False)

scopy = s.copy()
scopy[1] = 0.
reducedX = u @ np.diag(scopy) @ vt


ax = plt.figure(figsize = (7, 7)).add_subplot()
ut.centerAxes(ax)
plt.axis('equal')
plt.scatter(Xc[:,0],Xc[:,1], color='r')
plt.scatter(reducedX[:,0], reducedX[:,1])
endpoints = np.array([[-10],[10]]) @ vt[[0],:]
plt.plot(endpoints[:,0], endpoints[:,1], 'g-');
```
:::
::::

## Covariance Matrix
:::{style="font-size: .75em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box" style="background-color: #f3e6ff; padding: 10px; border-radius: 5px;">
        <p>
            Let \\( X \\in \\mathbb{R}^{m \\times n} \\) contain the centered data.
            Then the sample covariance matrix is defined by:
        </p>
        <p style="text-align: center; font-size: 1.2em;">
            \\[
            S = \\frac{1}{m-1} X^T X
            \\]
        </p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
$S$ is symmetric:

$$
S^T = \left( \frac{1}{m-1} X^T X \right)^T = \frac{1}{m-1} \left( X^T (X^T)^T \right) = \frac{1}{m-1} X^T X = S.
$$

Since $S$ is symmetric, we can apply the **Spectral Decomposition**.
:::

## Spectral Decomposition
:::{style="font-size: .65em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box" style="background-color: #f3e6ff; padding: 10px; border-radius: 5px;">
        <p>
            Every symmetric matrix \( S \) has the factorization \(S = V \Lambda V^T,\) where \( \Lambda \) is a diagonal matrix that contains the eigenvalues of \( S \) and \( V \) is a matrix whose columns are orthogonal eigenvectors of \( S \).
        </p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```

That is, the covariance matrix $S \in \mathbb{R}^{n \times n}$ has the spectral decomposition $S = V \Lambda V^T,$ where

$$
\Lambda = 
\begin{bmatrix}
\lambda_1                                   \\
& \lambda_2             &   & \Large0\\
&               & \ddots                \\
& \Large0 &   & \lambda_{n-1}            \\
&               &   &   & \lambda_n
\end{bmatrix}
\quad \text{and} \quad
V = \begin{bmatrix} 
\bigg| & \bigg| &  & \bigg| & \bigg| \\
\mathbf{v}_1   & \mathbf{v}_2  & \dots & \mathbf{v}_{n-1}   & \mathbf{v}_n  \\
\bigg| & \bigg| & & \bigg| & \bigg|
\end{bmatrix}
$$

:::{.center-text}
with $\lambda_1\geq\lambda_2\geq\dots\geq\lambda_{n-1}\geq\lambda_n$ and $S\mathbf{v}_i=\lambda_i\mathbf{v}_i$, $\mathbf{v}_i\perp\mathbf{v}_j$ for $i\neq j$.
:::

:::

## Essentials of PCA

:::{.incremental style="font-size: .9em"}
- The columns of $V$ are the principal directions.
- The principal components are the projection of the data into principal directions: $XV$.
- The total variance $T$ in the data is the sum of all eigenvalues: $T=\lambda_1+\lambda_2+\cdots+\lambda_n$.
- The first eigenvector $\mathbf{v}_1$ points in the most significant direction of the data. This direction explains the largest fraction $\lambda_1/T$ of the total variance.
- The second eigenvector $\mathbf{v}_2$ accounts for a smaller fraction $\lambda_2/T$.
:::

## Example

:::{style="font-size: .75em"}
:::{.fragment}
Consider the following dataset $\mathbf{x}_1 = \begin{bmatrix} 90 \\ 60 \\ 60 \end{bmatrix}$ and $\mathbf{x}_2 = \begin{bmatrix} 80 \\ 60 \\ 70 \end{bmatrix}$. Let us find its <br>**first principal component**.
:::
:::{.fragment}
_First_, we construct $X_0$: $X_0 = \begin{bmatrix} 90 & 60 & 60 \\ 80 & 60 & 70 \end{bmatrix}$.
:::
:::{.fragment}
The mean across rows is: $\mu = \begin{bmatrix} 85 & 60 & 65 \end{bmatrix}$.
:::
:::{.fragment}
:::{style="font-size: .90em"}
We _center_ the data by subtracting each entry of $\mu$ from the corresponding column in $X_0$:
:::
:::{.center-text}
$X = \begin{bmatrix} 5 & 0 & -5 \\ -5 & 0 & 5 \end{bmatrix}$.
:::
:::
:::

## Example

::::{style="font-size: .7em"}
Next, we compute the **covariance matrix:**

$$
S = \frac{1}{2-1}X^TX = \begin{bmatrix} 5 & -5 \\ 0 & 0 \\ -5 & 5 \end{bmatrix}
\begin{bmatrix} 5 & 0 & -5 \\ -5 & 0 & 5 \end{bmatrix}
= \begin{bmatrix} 50 & 0 & -50 \\ 0 & 0 & 0 \\ -50 & 0 & 50 \end{bmatrix}.
$$

The _eigenvalues_ of $S$ are $\lambda_1=100$ and $\lambda_2=\lambda_3=0$.

The _eigenvectors_ of $S$ are:

$$
\mathbf{v}_1 = \begin{bmatrix} 0.7071 \\ 0 \\ -0.7071 \end{bmatrix},
\mathbf{v}_2 = \begin{bmatrix} 0.7071 \\ 0 \\ 0.7071 \end{bmatrix},
\mathbf{v}_3 = \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}.
$$

:::{.center-text style="font-size: 1em"}
```{python}
from IPython.core.display import HTML

def generate_html():
    return """
    <div class="blue-box">
        <p><b>What is the total <em>variance</em> in the data?</b></p>
    </div>
    """

html_content = generate_html()
display(HTML(html_content))
```
:::
:::

## Example

The **total variance** is $T=100$.

:::{.fragment}
This implies that the _first principal component_ accounts for all the variance in the dataset.
:::
:::{.fragment}
We can compute the _first principal component_ as:

$$
Y = X\mathbf{v}_1 = \begin{bmatrix} 5 & 0 & -5 \\ -5 & 0 & 5 \end{bmatrix}
\begin{bmatrix} 0.7071 \\ 0 \\ -0.7071 \end{bmatrix}
= \begin{bmatrix} 7.071 \\ -7.071 \end{bmatrix}.
$$
:::

## PCA Step by Step
:::{style="font-size: .65em"}
Input: $X_0 \in \mathbb{R}^{m \times n}$  
Output: $Y \in \mathbb{R}^{m \times k}$ with $k \ll n$

:::{.fragment}
1. Center the data: $X_0 \to X$.
:::

:::{.fragment}
2. Compute the sample covariance matrix: $S = \frac{1}{m-1}X^TX$.
:::

:::{.fragment}
3. Compute the eigenvalues $\lambda_i$'s and the eigenvectors $\mathbf{v}_i$'s of $S$.
:::

:::{.fragment}
4. Select $k$ eigenvectors corresponding to the highest eigenvalues.
:::

:::{.fragment}
5. Assemble $V_k = \begin{bmatrix} | & | & \dots & | \\\mathbf{v}_1 & \mathbf{v}_2 & \dots & \mathbf{v}_k \\| & | &\dots & | \end{bmatrix}$.
:::

:::{.fragment}
6. Project the data: $Y = XV_k$.
:::
:::

## Relationship to the SVD

:::{style="font-size: .8em"}
The SVD of a mean-centered data matrix $X \in \mathbb{R}^{m \times n}$ is $X = U \Sigma V^T$.
<!-- Had to use an image instead of python code because of whitespace created -->
![](/images/pca/UeV.png)

:::{.center-text}
Hence, $X^TX = (U\Sigma V^T)^T(U\Sigma V^T) = (V^T)^T\Sigma U^TU\Sigma(V^T)^T$
:::
$$
= V\Sigma U^TU\Sigma V^T = V\Sigma I\Sigma V^T = V\Sigma^2 V^T.
$$
:::

## Relationship to the SVD

:::{style="font-size: 1.15em"}

:::{.fragment}
Recall that in PCA the covariance matrix $S$ is given by $\frac{1}{m-1} X^T X$.
:::
<br>

:::{.fragment}
Thus, $S = \frac{1}{m-1} V \Sigma^2 V^T$.
:::
<br>

:::{.fragment}
This tells us that the _eigenvalues_ $\lambda_i$ of $S$ are the **squares of the singular values** $\sigma_i^2$ of $\Sigma$, scaled by $\frac{1}{m-1}$.<br><br><br>
:::
:::

## Relationship to the SVD

In practice, PCA is done by computing the SVD of your data matrix as opposed to forming the covariance matrix and computing the eigenvalues. 

The principal components are the right singular vectors $V$. The projected data is computed as $XV$. The eigenvalues of $S$ are obtained from squaring the entries of $\Sigma$ and scaling them by $\frac{1}{m-1}$.

## Case Study: Random Data

Let's consider some randomly generated data consisting of 2 features.

```{python}
#| fig-align: center
import numpy as np
import matplotlib.pyplot as plt

rng = np.random.default_rng(seed=42)

import numpy as np

# Set the seed for reproducibility
seed = 42
rng = np.random.default_rng(seed)

n_samples = 500
C = np.array([[0.1, 0.6], [2., .6]])
X0 = rng.standard_normal((n_samples, 2)) @ C + np.array([-6, 3])
X = X0 - X0.mean(axis=0)

plt.scatter(X[:, 0], X[:, 1])
plt.axis('equal')
plt.show()
```

---

Let's do PCA and plot the principle components over our dataset. As expected, the principal components are orthogonal and point in the directions of the maximum variance.

```{python}
#| fig-align: center

import numpy as np
import matplotlib.pyplot as plt

origin = [0, 0]

# Compute the covariance matrix
cov_matrix = np.cov(X, rowvar=False)

# Calculate the eigenvalues and eigenvectors
eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)

# Sort the eigenvalues and eigenvectors in descending order
sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]

# Plot the original dataset
plt.scatter(X[:, 0], X[:, 1], alpha=0.2)

# Plot the principal components
for i in range(len(eigenvalues)):
    plt.quiver(origin[0], origin[1], -eigenvectors[0, i], -eigenvectors[1, i],
               angles='xy', scale_units='xy', scale=1, color=['r', 'g'][i])

plt.title('Principal Components on Original Dataset')
plt.axis('equal')
plt.show()
```

## Case Study: Digits

Let's consider another example using the MNIST dataset.

```{python}
#| fig-align: center
from sklearn import datasets
digits = datasets.load_digits()

plt.figure(figsize=(8, 8),)
for i in range(8):
    plt.subplot(2, 4, i + 1)
    plt.imshow(digits.images[i], cmap='gray_r')
    plt.axis('off')
plt.tight_layout()
plt.show()
```

---
:::{style="font-size: .8em"}
We first stack the columns of each digit on top of each other (this operation is called vectorizing) and perform PCA on the 64-D representation of the digits.

We can plot the explained variance ratio and the total (cumulative) explained variance ratio.
:::
```{python}
#| fig-align: center
from sklearn.decomposition import PCA

X = digits.data
y = digits.target

pca = PCA()
X_pca = pca.fit_transform(X)

# Create subplots
fig, axs = plt.subplots(2, 1, figsize=(6, 5))

# Scree plot (graph of eigenvalues corresponding to PC number)
# This shows the explained variance ratio
axs[0].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')
axs[0].set_title('Scree Plot')
axs[0].set_xlabel('Principal Component')
axs[0].set_ylabel('Explained Variance Ratio')
axs[0].grid(True)

# Cumulative explained variance plot
axs[1].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')
axs[1].set_title('Cumulative Explained Variance Plot')
axs[1].set_xlabel('Principal Component')
axs[1].set_ylabel('Cumulative Explained Variance')
axs[1].grid(True)

# Adjust layout
plt.tight_layout()
plt.show()
```

---
:::{style="font-size: .8em"}
Let's plot the data in the first 2 principal component directions. We'll use the digit labels to color each digit in the reduced space.
:::

```{python}
#| fig-align: center
plt.figure(figsize=(7, 7))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab20', edgecolor='k', s=50)
plt.title('Digits in PC1 and PC2 Space')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

# Create a legend with discrete labels
legend_labels = np.unique(y)
handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab20(i / 9), markersize=10) for i in legend_labels]
plt.legend(handles, legend_labels, title="Digit Label", loc="best")

plt.show()
```


---

We observe the following in our plot of the digits in the first two principal components:

- There is a decent clustering of some of our digits, in particular 0, 2, 3, 4, and 6.
- The numbers 0 and 6 seem to be relatively close to each other in this space.
- There is not a very clear separation of the number 5 from some of the other points.

## To Scale or Not to Scale
:::{style="font-size: .9em"}
Consider a situation where we have age and height data for 4 people.
:::

:::{.center-text style="max-width:60%;"}
![](images/pca/scale_and_table_feet.png)
:::

::::{.fragment}
:::{.center-text}
Data varies most in the **horizontal direction.**
:::
::::

## To Scale or Not to Scale

What if the height is in cm?

:::{.center-text style="max-width:70%;"}
![](images/pca/scale_and_tablecm.png)
:::

::::{.fragment}
:::{.center-text}
Data varies most in the **vertical direction.**
:::
::::
## To Scale or Not to Scale

Let's standardize our data.

:::{.center-text style="max-width:70%;"}
![](images/pca/standardized_graph_and_table.png)
:::

::::{.fragment}
:::{.center-text}
When we normalize our data, we observe an **equal distribution** of the variance.
:::
::::


--- 

What quantity is represented by $\frac{Cov(X, Y)}{\sigma_X\sigma_Y}$, where $X$ and $Y$ represent the random variables associated to sampling a person with that Age and Height?

:::: {.fragment}
This is the correlation matrix. When you standardize your data you are no longer working with the covariance matrix but the correlation matrix.
::::

:::: {.fragment}
PCA on a standardized dataset and working with the correlation matrix is the best practice when your data has very different scales. 

PCA identifies the directions with the maximum variance and large scale data can disproportionately influence the results of your PCA.
::::
